<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Modeling</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>



<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Modeling</h1>

</div>


<p>Previous file: 4-eda.Rmd</p>
<div id="load-data" class="section level2">
<h2>Load data</h2>
<pre class="r"><code># Created in 3-clean-finalize.Rmd
# Objects included: data, vars, var_df
# renv also includes a load() method, so we specify base:: here.
base::load(&quot;data/clean-finalize-imputed.RData&quot;)</code></pre>
</div>
<div id="random-forest" class="section level2">
<h2>Random forest</h2>
<pre class="r"><code>set.seed(1, &quot;L&#39;Ecuyer-CMRG&quot;)

# mlr wants covariates and outcome to be in the same dataframe.

# For classification RF needs Y to be a factor.
# We use the best mtry based on the CV.SL results from the final prediction library.
# Takes 1 second.
(rf_time = system.time({
  # Ranger uses all available threads by default, nice.
  y = as.factor(data[[vars$outcomes[1]]]) 
  rf = ranger::ranger(y ~ . ,
                      data = data[, vars$predictors],
                      num.threads = get_cores(),
                      # Need this option for OOB curve analysis.
                      keep.inbag = TRUE,
                      num.trees = 4000,
                      # Could also do importance = &quot;impurity&quot;.
                      importance = &quot;permutation&quot;,
                      # Set based on separate grid/random search.
                      mtry = 4L,
                      # Set based on separate grid/random search.
                      min.node.size = 5L)
}))</code></pre>
<pre><code>##    user  system elapsed 
##    1.68    0.19    0.72</code></pre>
<pre class="r"><code>save(rf, file = &quot;data/model-rf.RData&quot;)</code></pre>
<pre class="r"><code>library(mlr)
library(OOBCurve)

oob_data = data[, c(vars$outcomes[1], vars$predictors), drop = FALSE]

# Outcome needs to be a factor.
oob_data[[vars$outcomes[1]]] = as.factor(data[[vars$outcomes[1]]])


task = makeClassifTask(data = oob_data, target = vars$outcomes[1])
# Current package has a bug such that multiple measures have to be specified.
# We aren&#39;t using the Brier score though.
# TODO: these results could be averaged over multiple random shufflings
# of the tree ordering. Would give a more accurate, smoother curve.
# This takes ~10 seconds.
system.time({
  results = OOBCurve(rf, measures = list(mlr::auc, mlr::brier), task = task,
                     data = oob_data)
})</code></pre>
<pre><code>##    user  system elapsed 
##    7.78    0.09    7.69</code></pre>
<pre class="r"><code># Look at the OOB AUC with the maximum number of trees.
# 0.894
(rf_auc = results$auc[length(results$auc)])</code></pre>
<pre><code>## [1] 0.8943347</code></pre>
<pre class="r"><code># Can zoom in to certain segments of the forest indexed by an ntree range.
tree_start = 3
#tree_start = 10
tree_end = length(results$auc)
x_span = seq(tree_start, tree_end)
y_span = results$auc[x_span]

ggplot(mapping = aes(x = x_span, y = y_span)) + geom_line() + theme_minimal() +
  coord_fixed(ratio = 3) + 
  scale_y_continuous(expand = c(0, 0), limits = c(0.5, 0.94)) +
  scale_x_log10(breaks = c(3, 10, 30, 100, 300, 1000, 3000),
                limits = c(3, 3000),
                minor_breaks = NULL) +
  labs(x = &quot;Trees in the random forest&quot;, y = &quot;Out of Bag AUC&quot;)</code></pre>
<pre><code>## Warning: Removed 1000 row(s) containing missing values (geom_path).</code></pre>
<p><img src="5-modeling_files/figure-html/rf_oob_curve-1.png" width="672" /></p>
<pre class="r"><code>ggsave(&quot;visuals/rf-error-rate-by-trees.pdf&quot;,
       width = 7, height = 3)</code></pre>
<pre><code>## Warning: Removed 1000 row(s) containing missing values (geom_path).</code></pre>
</div>
<div id="ensemble" class="section level2">
<h2>Ensemble</h2>
<div id="prep-sl-library" class="section level3">
<h3>Prep SL library</h3>
<pre class="r"><code># Via R/sl-dbarts2.R
SL.dbarts = SL.dbarts2

learner_bart =
  create.Learner(&quot;SL.dbarts&quot;,
                 # Turning off detailed_names because binary_offset has a negative value.
                 #detailed_names = FALSE,
                 detailed_names = TRUE,
                 params = list(nthread = getOption(&quot;cores&quot;)),
                 tune = list(ntree = c(1, 2, 5, 10, 20, 50, 100, 200, 500)))#,

screen_names = function(Y, X, names = names(X), ...)  {
    return(names(X) %in% names)
}


# TODO: add feature selection options (BART, lasso, RF)
# TODO: explore optimization of the meta-learner step.
xgb_tune = function(Y, X, newX, family, obsWeights, id, ...) {
  cat(&quot;Running xgb_tune\n&quot;)
  
  # Create tuning grid.
  grid = create.Learner(&quot;SL.xgboost_fast&quot;, detailed_names = TRUE, 
                        tune = list(
                          # 27 combos.
                          ntrees = c(100L, 300L, 1000L),
                          max_depth = c(1L, 3L, 6L),
                          shrinkage = c(0.01, 0.1, 0.3)))
  
  grid2 = create.Learner(&quot;SL.xgboost_fast&quot;, detailed_names = TRUE, 
                        tune = list(
                          # 8 combos.
                          ntrees = c(250L, 1000L),
                          max_depth = c(2L, 4L),
                          shrinkage = c(0.05, 0.2)))
  
  # Run SuperLearner.
  # id argument is not being passed to avoid an error &quot;stratified sampling with id not currently implemented&quot;
  sl = SuperLearner(Y = Y, X = X, newX = newX, obsWeights = obsWeights, family = family,
                    SL.library = grid2$names,
                    cvControl = SuperLearner.CV.control(stratifyCV = TRUE, V = 5L),
                    verbose = FALSE)
  
  cat(&quot;XGB tuned SL:\n&quot;)
  print(sl)
  
  # fit returns all objects needed for predict()
  fit = list(object = sl)
  
  # Declare class of fit for predict()
  class(fit) = &#39;SuperLearner&#39;
  
  out = list(pred = sl$SL.predict, fit = fit)
  return(out)
}

rf_tune = function(Y, X, newX, family, obsWeights, id, ...) {
  cat(&quot;Running rf_tune\n&quot;)
  
  # Create tuning grid.
  grid = create.Learner(&quot;SL.ranger&quot;, detailed_names = TRUE, 
                        params = list(num.threads = get_cores(),
                                      #num.trees = 700),
                                      num.trees = 200),
                        tune = list(
                          # 9 combos.
                          min.node.size = c(2L, 5L, 15L),
                          mtry = floor(c(0.5, 1, 2) * sqrt(ncol(X)))))
  
  # Run SuperLearner.
  # id argument is not being passed to avoid an error &quot;stratified sampling with id not currently implemented&quot;
  sl = SuperLearner(Y = Y, X = X, newX = newX, obsWeights = obsWeights, family = family,
                    #SL.library = c(&quot;SL.mean&quot;, grid$names),
                    SL.library = grid$names,
                    cvControl = SuperLearner.CV.control(stratifyCV = TRUE, V = 5L),
                    verbose = FALSE)
  
  cat(&quot;RF tuned SL:\n&quot;)
  print(sl)
  
  # fit returns all objects needed for predict()
  fit = list(object = sl)
  
  # Declare class of fit for predict()
  class(fit) = &#39;SuperLearner&#39;
  
  out = list(pred = sl$SL.predict, fit = fit)
  return(out)
}

glmnet_tune = function(Y, X, newX, family, obsWeights, id, ...) {
  cat(&quot;Running glmnet_tune\n&quot;)
  
  # Create tuning grid.
  grid = create.Learner(&quot;SL.glmnet_fast&quot;, detailed_names = TRUE, 
                        # 4 combos
                        tune = list(alpha = c(0.05, 0.3, 0.7, 0.95)))
  
  # Run SuperLearner.
  # id argument is not being passed to avoid an error &quot;stratified sampling with id not currently implemented&quot;
  sl = SuperLearner(Y = Y, X = X, newX = newX, obsWeights = obsWeights, family = family,
                    #SL.library = c(&quot;SL.mean&quot;, grid$names),
                    SL.library = grid$names,
                    cvControl = SuperLearner.CV.control(stratifyCV = TRUE, V = 5L),
                    verbose = FALSE)
  
  cat(&quot;Glmnet tuned SL:\n&quot;)
  print(sl)
  
  # TODO: may need to save the learners, put them in the global environment, or otherwise
  # handle in a custom predict() method.
  
  # fit returns all objects needed for predict()
  fit = list(object = sl)
  
  # Declare class of fit for predict()
  class(fit) = &#39;SuperLearner&#39;
  
  out = list(pred = sl$SL.predict, fit = fit)
  return(out)
}

rpart_tune = function(Y, X, newX, family, obsWeights, id, ...) {
  cat(&quot;Running rpart_tune\n&quot;)
  
  # Create tuning grid.
  grid = create.Learner(&quot;SL.rpart2&quot;, detailed_names = TRUE, 
                        tune = list(cp = c(0, 0.01),
                                    minsplit = c(10, 20, 80),
                                    maxdepth = c(5, 15)))
  
  # Run SuperLearner.
  # id argument is not being passed to avoid an error &quot;stratified sampling with id not currently implemented&quot;
  sl = SuperLearner(Y = Y, X = X, newX = newX, obsWeights = obsWeights, family = family,
                    #SL.library = c(&quot;SL.mean&quot;, grid$names),
                    SL.library = grid$names,
                    cvControl = SuperLearner.CV.control(stratifyCV = TRUE, V = 5L),
                    verbose = FALSE)
  
  cat(&quot;Rpart tuned SL:\n&quot;)
  print(sl)
  
  # TODO: may need to save the learners, put them in the global environment, or otherwise
  # handle in a custom predict() method.
  
  # fit returns all objects needed for predict()
  fit = list(object = sl)
  
  # Declare class of fit for predict()
  class(fit) = &#39;SuperLearner&#39;
  
  out = list(pred = sl$SL.predict, fit = fit)
  return(out)
}

learner_mgcv =
  create.Learner(&quot;SL.mgcv2&quot;,
                 detailed_names = TRUE,
                 params = list(nthreads = min(10L, get_cores()),
                               continuous_values = 10L))


(sl_lib = c(list(&quot;SL.mean&quot;,
                 &quot;SL.lm2&quot;,
                 &quot;SL.glm2&quot;,
                 &quot;SL.glmnet_fast&quot;),
#           stratified_lib$names, 
#           rpart_pruned$names,
           learner_mgcv$names,
          list(
            &quot;rpart_tune&quot;,
            &quot;SL.ranger_200&quot;,
            &quot;rf_tune&quot;,
            &quot;SL.dbarts_200&quot;,
            &quot;xgb_tune&quot;,
            &quot;SL.xgboost_fast&quot;)))</code></pre>
<pre><code>## [[1]]
## [1] &quot;SL.mean&quot;
## 
## [[2]]
## [1] &quot;SL.lm2&quot;
## 
## [[3]]
## [1] &quot;SL.glm2&quot;
## 
## [[4]]
## [1] &quot;SL.glmnet_fast&quot;
## 
## [[5]]
## [1] &quot;SL.mgcv2_1&quot;
## 
## [[6]]
## [1] &quot;rpart_tune&quot;
## 
## [[7]]
## [1] &quot;SL.ranger_200&quot;
## 
## [[8]]
## [1] &quot;rf_tune&quot;
## 
## [[9]]
## [1] &quot;SL.dbarts_200&quot;
## 
## [[10]]
## [1] &quot;xgb_tune&quot;
## 
## [[11]]
## [1] &quot;SL.xgboost_fast&quot;</code></pre>
</div>
<div id="estimate-superlearner" class="section level3">
<h3>Estimate SuperLearner</h3>
<p>The results of this block are cached because they are slow to compute.</p>
<pre class="r"><code>set.seed(1, &quot;L&#39;Ecuyer-CMRG&quot;)

(sl = SuperLearner(Y = data[[vars$outcomes[1]]], data[, vars$predictors],
                  family = binomial(), SL.library = sl_lib,
                  cvControl = SuperLearner.CV.control(stratifyCV = TRUE,
                                                      #V = 10L),
                                                      V = 2L),
                  verbose = TRUE))</code></pre>
<pre><code>## Number of covariates in All is: 33</code></pre>
<pre><code>## CV SL.mean_All</code></pre>
<pre><code>## Warning in predict.lm(fit, newdata = newX, type = &quot;response&quot;): prediction from a
## rank-deficient fit may be misleading</code></pre>
<pre><code>## CV SL.lm2_All</code></pre>
<pre><code>## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == :
## prediction from a rank-deficient fit may be misleading</code></pre>
<pre><code>## CV SL.glm2_All</code></pre>
<pre><code>## Warning: executing %dopar% sequentially: no parallel backend registered</code></pre>
<pre><code>## CV SL.glmnet_fast_All</code></pre>
<pre><code>## Warning in newton(lsp = lsp, X = G$X, y = G$y, Eb = G$Eb, UrS = G$UrS, L =
## G$L, : Fitting terminated with step failure - check results carefully</code></pre>
<pre><code>## CV SL.mgcv2_1_All</code></pre>
<pre><code>## Running rpart_tune
## Rpart tuned SL:
## 
## Call:  
## SuperLearner(Y = Y, X = X, newX = newX, family = family, SL.library = grid$names,  
##     verbose = FALSE, cvControl = SuperLearner.CV.control(stratifyCV = TRUE,  
##         V = 5L), obsWeights = obsWeights) 
## 
## 
##                               Risk      Coef
## SL.rpart2_0_10_5_All     0.1940270 0.0000000
## SL.rpart2_0.01_10_5_All  0.1940270 0.1767767
## SL.rpart2_0_20_5_All     0.1570279 0.0000000
## SL.rpart2_0.01_20_5_All  0.1533203 0.6261408
## SL.rpart2_0_80_5_All     0.1683419 0.1970825
## SL.rpart2_0.01_80_5_All  0.1683419 0.0000000
## SL.rpart2_0_10_15_All    0.1940270 0.0000000
## SL.rpart2_0.01_10_15_All 0.1940270 0.0000000
## SL.rpart2_0_20_15_All    0.1570279 0.0000000
## SL.rpart2_0.01_20_15_All 0.1533203 0.0000000
## SL.rpart2_0_80_15_All    0.1683419 0.0000000
## SL.rpart2_0.01_80_15_All 0.1683419 0.0000000</code></pre>
<pre><code>## CV rpart_tune_All</code></pre>
<pre><code>## CV SL.ranger_200_All</code></pre>
<pre><code>## Running rf_tune
## RF tuned SL:
## 
## Call:  
## SuperLearner(Y = Y, X = X, newX = newX, family = family, SL.library = grid$names,  
##     verbose = FALSE, cvControl = SuperLearner.CV.control(stratifyCV = TRUE,  
##         V = 5L), obsWeights = obsWeights) 
## 
## 
##                          Risk      Coef
## SL.ranger_2_2_All   0.1443120 0.3466695
## SL.ranger_5_2_All   0.1467623 0.0000000
## SL.ranger_15_2_All  0.1489224 0.0000000
## SL.ranger_2_5_All   0.1392814 0.2724390
## SL.ranger_5_5_All   0.1406905 0.0000000
## SL.ranger_15_5_All  0.1417578 0.0000000
## SL.ranger_2_11_All  0.1417951 0.0000000
## SL.ranger_5_11_All  0.1404820 0.3808915
## SL.ranger_15_11_All 0.1409560 0.0000000</code></pre>
<pre><code>## CV rf_tune_All</code></pre>
<pre><code>## CV SL.dbarts_200_All</code></pre>
<pre><code>## Running xgb_tune
## XGB tuned SL:
## 
## Call:  
## SuperLearner(Y = Y, X = X, newX = newX, family = family, SL.library = grid2$names,  
##     verbose = FALSE, cvControl = SuperLearner.CV.control(stratifyCV = TRUE,  
##         V = 5L), obsWeights = obsWeights) 
## 
## 
##                                      Risk Coef
## SL.xgboost_fast_250_2_0.05_All  0.1587276    1
## SL.xgboost_fast_1000_2_0.05_All 0.1591445    0
## SL.xgboost_fast_250_4_0.05_All  0.1587276    0
## SL.xgboost_fast_1000_4_0.05_All 0.1591445    0
## SL.xgboost_fast_250_2_0.2_All   0.1600775    0
## SL.xgboost_fast_1000_2_0.2_All  0.1600775    0
## SL.xgboost_fast_250_4_0.2_All   0.1600775    0
## SL.xgboost_fast_1000_4_0.2_All  0.1600775    0</code></pre>
<pre><code>## CV xgb_tune_All</code></pre>
<pre><code>## CV SL.xgboost_fast_All</code></pre>
<pre><code>## Number of covariates in All is: 33</code></pre>
<pre><code>## CV SL.mean_All</code></pre>
<pre><code>## CV SL.lm2_All</code></pre>
<pre><code>## CV SL.glm2_All</code></pre>
<pre><code>## CV SL.glmnet_fast_All</code></pre>
<pre><code>## Warning in newton(lsp = lsp, X = G$X, y = G$y, Eb = G$Eb, UrS = G$UrS, L =
## G$L, : Fitting terminated with step failure - check results carefully</code></pre>
<pre><code>## CV SL.mgcv2_1_All</code></pre>
<pre><code>## Running rpart_tune
## Rpart tuned SL:
## 
## Call:  
## SuperLearner(Y = Y, X = X, newX = newX, family = family, SL.library = grid$names,  
##     verbose = FALSE, cvControl = SuperLearner.CV.control(stratifyCV = TRUE,  
##         V = 5L), obsWeights = obsWeights) 
## 
## 
##                               Risk       Coef
## SL.rpart2_0_10_5_All     0.2271647 0.09770921
## SL.rpart2_0.01_10_5_All  0.2277811 0.00000000
## SL.rpart2_0_20_5_All     0.2108773 0.43735684
## SL.rpart2_0.01_20_5_All  0.2108773 0.00000000
## SL.rpart2_0_80_5_All     0.2135859 0.46493395
## SL.rpart2_0.01_80_5_All  0.2135859 0.00000000
## SL.rpart2_0_10_15_All    0.2386639 0.00000000
## SL.rpart2_0.01_10_15_All 0.2392804 0.00000000
## SL.rpart2_0_20_15_All    0.2108773 0.00000000
## SL.rpart2_0.01_20_15_All 0.2108773 0.00000000
## SL.rpart2_0_80_15_All    0.2135859 0.00000000
## SL.rpart2_0.01_80_15_All 0.2135859 0.00000000</code></pre>
<pre><code>## CV rpart_tune_All</code></pre>
<pre><code>## CV SL.ranger_200_All</code></pre>
<pre><code>## Running rf_tune
## RF tuned SL:
## 
## Call:  
## SuperLearner(Y = Y, X = X, newX = newX, family = family, SL.library = grid$names,  
##     verbose = FALSE, cvControl = SuperLearner.CV.control(stratifyCV = TRUE,  
##         V = 5L), obsWeights = obsWeights) 
## 
## 
##                          Risk      Coef
## SL.ranger_2_2_All   0.1523043 0.0000000
## SL.ranger_5_2_All   0.1515386 0.0000000
## SL.ranger_15_2_All  0.1533921 0.0000000
## SL.ranger_2_5_All   0.1382997 0.5081748
## SL.ranger_5_5_All   0.1410666 0.0000000
## SL.ranger_15_5_All  0.1438017 0.0000000
## SL.ranger_2_11_All  0.1407414 0.0000000
## SL.ranger_5_11_All  0.1379665 0.4918252
## SL.ranger_15_11_All 0.1453940 0.0000000</code></pre>
<pre><code>## CV rf_tune_All</code></pre>
<pre><code>## CV SL.dbarts_200_All</code></pre>
<pre><code>## Running xgb_tune
## XGB tuned SL:
## 
## Call:  
## SuperLearner(Y = Y, X = X, newX = newX, family = family, SL.library = grid2$names,  
##     verbose = FALSE, cvControl = SuperLearner.CV.control(stratifyCV = TRUE,  
##         V = 5L), obsWeights = obsWeights) 
## 
## 
##                                      Risk Coef
## SL.xgboost_fast_250_2_0.05_All  0.1763520    0
## SL.xgboost_fast_1000_2_0.05_All 0.1766191    0
## SL.xgboost_fast_250_4_0.05_All  0.1763520    0
## SL.xgboost_fast_1000_4_0.05_All 0.1766191    0
## SL.xgboost_fast_250_2_0.2_All   0.1748412    1
## SL.xgboost_fast_1000_2_0.2_All  0.1748627    0
## SL.xgboost_fast_250_4_0.2_All   0.1748412    0
## SL.xgboost_fast_1000_4_0.2_All  0.1748627    0</code></pre>
<pre><code>## CV xgb_tune_All</code></pre>
<pre><code>## CV SL.xgboost_fast_All</code></pre>
<pre><code>## Non-Negative least squares convergence: TRUE</code></pre>
<pre><code>## full SL.mean_All</code></pre>
<pre><code>## full SL.lm2_All</code></pre>
<pre><code>## full SL.glm2_All</code></pre>
<pre><code>## full SL.glmnet_fast_All</code></pre>
<pre><code>## full SL.mgcv2_1_All</code></pre>
<pre><code>## Running rpart_tune
## Rpart tuned SL:
## 
## Call:  
## SuperLearner(Y = Y, X = X, newX = newX, family = family, SL.library = grid$names,  
##     verbose = FALSE, cvControl = SuperLearner.CV.control(stratifyCV = TRUE,  
##         V = 5L), obsWeights = obsWeights) 
## 
## 
##                               Risk       Coef
## SL.rpart2_0_10_5_All     0.2019685 0.44859200
## SL.rpart2_0.01_10_5_All  0.2058342 0.00000000
## SL.rpart2_0_20_5_All     0.2032647 0.00000000
## SL.rpart2_0.01_20_5_All  0.1963048 0.03968870
## SL.rpart2_0_80_5_All     0.1946454 0.45969395
## SL.rpart2_0.01_80_5_All  0.1968973 0.05202535
## SL.rpart2_0_10_15_All    0.2156259 0.00000000
## SL.rpart2_0.01_10_15_All 0.2166788 0.00000000
## SL.rpart2_0_20_15_All    0.2032647 0.00000000
## SL.rpart2_0.01_20_15_All 0.1963048 0.00000000
## SL.rpart2_0_80_15_All    0.1946454 0.00000000
## SL.rpart2_0.01_80_15_All 0.1968973 0.00000000</code></pre>
<pre><code>## full rpart_tune_All</code></pre>
<pre><code>## full SL.ranger_200_All</code></pre>
<pre><code>## Running rf_tune
## RF tuned SL:
## 
## Call:  
## SuperLearner(Y = Y, X = X, newX = newX, family = family, SL.library = grid$names,  
##     verbose = FALSE, cvControl = SuperLearner.CV.control(stratifyCV = TRUE,  
##         V = 5L), obsWeights = obsWeights) 
## 
## 
##                          Risk       Coef
## SL.ranger_2_2_All   0.1389171 0.00983886
## SL.ranger_5_2_All   0.1398045 0.00000000
## SL.ranger_15_2_All  0.1407178 0.00000000
## SL.ranger_2_5_All   0.1328743 0.36141939
## SL.ranger_5_5_All   0.1326392 0.62874175
## SL.ranger_15_5_All  0.1342676 0.00000000
## SL.ranger_2_11_All  0.1382445 0.00000000
## SL.ranger_5_11_All  0.1385012 0.00000000
## SL.ranger_15_11_All 0.1390932 0.00000000</code></pre>
<pre><code>## full rf_tune_All</code></pre>
<pre><code>## full SL.dbarts_200_All</code></pre>
<pre><code>## Running xgb_tune
## XGB tuned SL:
## 
## Call:  
## SuperLearner(Y = Y, X = X, newX = newX, family = family, SL.library = grid2$names,  
##     verbose = FALSE, cvControl = SuperLearner.CV.control(stratifyCV = TRUE,  
##         V = 5L), obsWeights = obsWeights) 
## 
## 
##                                      Risk       Coef
## SL.xgboost_fast_250_2_0.05_All  0.1458468 0.84152937
## SL.xgboost_fast_1000_2_0.05_All 0.1476336 0.00000000
## SL.xgboost_fast_250_4_0.05_All  0.1469634 0.00000000
## SL.xgboost_fast_1000_4_0.05_All 0.1479619 0.00000000
## SL.xgboost_fast_250_2_0.2_All   0.1477140 0.03199365
## SL.xgboost_fast_1000_2_0.2_All  0.1495422 0.12647698
## SL.xgboost_fast_250_4_0.2_All   0.1478853 0.00000000
## SL.xgboost_fast_1000_4_0.2_All  0.1501961 0.00000000</code></pre>
<pre><code>## full xgb_tune_All</code></pre>
<pre><code>## full SL.xgboost_fast_All</code></pre>
<pre><code>## 
## Call:  
## SuperLearner(Y = data[[vars$outcomes[1]]], X = data[, vars$predictors], family = binomial(),  
##     SL.library = sl_lib, verbose = TRUE, cvControl = SuperLearner.CV.control(stratifyCV = TRUE,  
##         V = 2L)) 
## 
## 
##                          Risk        Coef
## SL.mean_All         0.2480217 0.000000000
## SL.lm2_All          0.1234965 0.194441050
## SL.glm2_All         0.1594002 0.001258443
## SL.glmnet_fast_All  0.1203540 0.353741680
## SL.mgcv2_1_All      0.2107545 0.000000000
## rpart_tune_All      0.1797903 0.000000000
## SL.ranger_200_All   0.1341187 0.000000000
## rf_tune_All         0.1357850 0.000000000
## SL.dbarts_200_All   0.1217067 0.450558827
## xgb_tune_All        0.1514361 0.000000000
## SL.xgboost_fast_All 0.1548813 0.000000000</code></pre>
<pre class="r"><code>sl</code></pre>
<pre><code>## 
## Call:  
## SuperLearner(Y = data[[vars$outcomes[1]]], X = data[, vars$predictors], family = binomial(),  
##     SL.library = sl_lib, verbose = TRUE, cvControl = SuperLearner.CV.control(stratifyCV = TRUE,  
##         V = 2L)) 
## 
## 
##                          Risk        Coef
## SL.mean_All         0.2480217 0.000000000
## SL.lm2_All          0.1234965 0.194441050
## SL.glm2_All         0.1594002 0.001258443
## SL.glmnet_fast_All  0.1203540 0.353741680
## SL.mgcv2_1_All      0.2107545 0.000000000
## rpart_tune_All      0.1797903 0.000000000
## SL.ranger_200_All   0.1341187 0.000000000
## rf_tune_All         0.1357850 0.000000000
## SL.dbarts_200_All   0.1217067 0.450558827
## xgb_tune_All        0.1514361 0.000000000
## SL.xgboost_fast_All 0.1548813 0.000000000</code></pre>
<pre class="r"><code>save(sl,
     file = &quot;data/estimator-sl.RData&quot;)</code></pre>
</div>
<div id="review-sl-results" class="section level3">
<h3>Review SL results</h3>
<pre class="r"><code>(auc_tab = ck37r::auc_table(sl, y = data[[vars$outcomes[1]]]))</code></pre>
<pre><code>##                learner       auc         se  ci_lower  ci_upper      p-value
## 1          SL.mean_All 0.5000000 0.05767800 0.3869532 0.6130468 6.710161e-13
## 5       SL.mgcv2_1_All 0.7926052 0.03489829 0.7242058 0.8610046 4.291155e-04
## 6       rpart_tune_All 0.8009376 0.03234135 0.7375497 0.8643255 4.201103e-04
## 3          SL.glm2_All 0.8500023 0.02405759 0.8028503 0.8971544 7.152594e-03
## 11 SL.xgboost_fast_All 0.8513109 0.02223601 0.8077291 0.8948926 4.780362e-03
## 10        xgb_tune_All 0.8544150 0.02215483 0.8109923 0.8978377 6.933143e-03
## 8          rf_tune_All 0.8907230 0.01809063 0.8552660 0.9261800 1.570829e-01
## 7    SL.ranger_200_All 0.8929408 0.01770943 0.8582310 0.9276506 1.832753e-01
## 4   SL.glmnet_fast_All 0.9021138 0.01826855 0.8663081 0.9379194 3.544996e-01
## 2           SL.lm2_All 0.9036720 0.01845892 0.8674931 0.9398508 3.878460e-01
## 9    SL.dbarts_200_All 0.9089316 0.01703545 0.8755427 0.9423205 5.000000e-01</code></pre>
<pre class="r"><code># Drop p-value column.
auc_tab2 = auc_tab[, !names(auc_tab) %in% &quot;p-value&quot;]


# TODO: convert to knitr/kableExtra
print(xtable::xtable(auc_tab2, digits = 4), type = &quot;latex&quot;,
      file = &quot;tables/sl-auc_table.tex&quot;)

ck37r::plot_roc(sl, y = data[[vars$outcomes[1]]])</code></pre>
<p><img src="5-modeling_files/figure-html/review_sl-1.png" width="672" /></p>
<pre class="r"><code>ggsave(&quot;visuals/roc-superlearner.pdf&quot;)</code></pre>
<pre><code>## Saving 7 x 5 in image</code></pre>
<pre class="r"><code>plot_table = function(x,
                      metric = &quot;auc&quot;,
                      sort = TRUE) {

  # Use a clearer object name.
  tab = x

  if (!is.null(sort)) {
    tab = tab[order(tab[[metric]], decreasing = sort), ]
  }

  # Convert to a factor with manual levels so ggplot doesn&#39;t re-order
  # alphabetically.
  tab$learner = factor(tab$learner, levels = tab$learner)

  rownames(tab) = NULL

  p =
    ggplot2::ggplot(tab,
           aes(x = learner, y = get(metric), ymin = ci_lower, ymax = ci_upper)) +
      ggplot2::geom_pointrange(fatten = 2) +
      ggplot2::coord_flip() +
      ggplot2::labs(x = &quot;Learner&quot;, y = metric) + theme_minimal()

  return(p)
}

# Skip SL.mean - it&#39;s too low to be worth plotting.
plot_table(auc_tab[-1, ]) + labs(y = &quot;Cross-validated ROC-AUC&quot;)</code></pre>
<p><img src="5-modeling_files/figure-html/review_sl-2.png" width="672" /></p>
<pre class="r"><code>ggsave(&quot;visuals/sl-roc-auc-comparison.pdf&quot;)</code></pre>
<pre><code>## Saving 7 x 5 in image</code></pre>
</div>
<div id="sl-pr-auc" class="section level3">
<h3>SL PR-AUC</h3>
<pre class="r"><code>(prauc_tab = ck37r::prauc_table(sl, y = data[[vars$outcomes[1]]]))

# TODO: switch to knitr
print(xtable::xtable(prauc_tab, digits = 4), type = &quot;latex&quot;,
      file = &quot;tables/sl-prauc_table.tex&quot;)

plot_table(prauc_tab, metric = &quot;prauc&quot;) + labs(y = &quot;Cross-validated PR-AUC&quot;)
ggsave(&quot;visuals/sl-prauc-comparison.pdf&quot;)</code></pre>
</div>
<div id="sl-plots" class="section level3">
<h3>SL plots</h3>
<pre class="r"><code>library(dplyr)

df = data.frame(y = data[[vars$outcomes[1]]],
                pred = as.vector(sl$SL.predict))

df = df %&gt;% mutate(decile = ntile(pred, 10L),
                   vigintile = ntile(pred, 20L)) %&gt;% as.data.frame()

table(df$decile)

summary(df)

# Compare risk distribution for 0&#39;s vs 1&#39;s
ggplot(data = df, aes(x = pred, color = factor(y))) + 
  geom_density() + theme_minimal() +
  labs(title = &quot;Distribution of predicted risk for 0&#39;s vs 1&#39;s&quot;,
         x = &quot;Predicted risk Pr(Y = 1 | X)&quot;,
         y = &quot;Density&quot;)

# Look at the risk distribution for each learner.
for (learner_i in seq(ncol(sl$Z))) {
  learner_name = sl$libraryNames[learner_i]
  preds = sl$Z[, learner_i, drop = TRUE]
  
  df = data.frame(y = data[[vars$outcomes[1]]],
                  pred = preds)
  
  g = ggplot(data = df, aes(x = pred, color = factor(y))) + 
    geom_density() + theme_minimal() +
    labs(title = &quot;Distribution of predicted risk for 0&#39;s vs 1&#39;s&quot;,
         subtitle = paste(&quot;Learner:&quot;, learner_name), 
         x = &quot;Predicted risk Pr(Y = 1 | X)&quot;,
         y = &quot;Density&quot;)
  
  print(g)

}


ggplot(data = df, aes(x = pred, color = factor(y))) + 
  geom_freqpoly() + theme_minimal()

# Quick calibration plot
ggplot(data = df, aes(x = pred, y = y)) + 
  geom_smooth() + theme_minimal() +
  lims(y = c(0, 1))</code></pre>
</div>
</div>
<div id="nested-ensemble" class="section level2">
<h2>Nested ensemble</h2>
<p>The results of this block are cached because they are slow to compute.</p>
<p>Note: we are not currently saving the fitLibraries.</p>
<pre class="r"><code>set.seed(1, &quot;L&#39;Ecuyer-CMRG&quot;)

# 2 is fastest, 10 is most thorouugh.
#outer_cv_folds = 10L
#outer_cv_folds = 5L
outer_cv_folds = 2L

(cvsl =
    CV.SuperLearner(Y = data[[vars$outcomes[1]]], data[, vars$predictors],
          family = binomial(), SL.library = sl_lib,
          cvControl =
            SuperLearner.CV.control(stratifyCV = TRUE,
                                    V = outer_cv_folds),
          innerCvControl =
            rep(list(SuperLearner.CV.control(stratifyCV = TRUE,
                                             V = 2L)),
                                             #V = 5L)),
                                             #V = 10L)),
                outer_cv_folds),
          verbose = TRUE))</code></pre>
<pre><code>## Number of covariates in All is: 33</code></pre>
<pre><code>## CV SL.mean_All</code></pre>
<pre><code>## Warning in predict.lm(fit, newdata = newX, type = &quot;response&quot;): prediction from a
## rank-deficient fit may be misleading</code></pre>
<pre><code>## CV SL.lm2_All</code></pre>
<pre><code>## Warning: glm.fit: algorithm did not converge</code></pre>
<pre><code>## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred</code></pre>
<pre><code>## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == :
## prediction from a rank-deficient fit may be misleading</code></pre>
<pre><code>## CV SL.glm2_All</code></pre>
<pre><code>## CV SL.glmnet_fast_All</code></pre>
<pre><code>## Warning in newton(lsp = lsp, X = G$X, y = G$y, Eb = G$Eb, UrS = G$UrS, L =
## G$L, : Fitting terminated with step failure - check results carefully</code></pre>
<pre><code>## CV SL.mgcv2_1_All</code></pre>
<pre><code>## Running rpart_tune
## Rpart tuned SL:
## 
## Call:  
## SuperLearner(Y = Y, X = X, newX = newX, family = family, SL.library = grid$names,  
##     verbose = FALSE, cvControl = SuperLearner.CV.control(stratifyCV = TRUE,  
##         V = 5L), obsWeights = obsWeights) 
## 
## 
##                               Risk      Coef
## SL.rpart2_0_10_5_All     0.2431768 0.0000000
## SL.rpart2_0.01_10_5_All  0.2431768 0.0000000
## SL.rpart2_0_20_5_All     0.1930073 0.8053982
## SL.rpart2_0.01_20_5_All  0.1930073 0.0000000
## SL.rpart2_0_80_5_All     0.2479737 0.1946018
## SL.rpart2_0.01_80_5_All  0.2479737 0.0000000
## SL.rpart2_0_10_15_All    0.2431768 0.0000000
## SL.rpart2_0.01_10_15_All 0.2431768 0.0000000
## SL.rpart2_0_20_15_All    0.1930073 0.0000000
## SL.rpart2_0.01_20_15_All 0.1930073 0.0000000
## SL.rpart2_0_80_15_All    0.2479737 0.0000000
## SL.rpart2_0.01_80_15_All 0.2479737 0.0000000</code></pre>
<pre><code>## CV rpart_tune_All</code></pre>
<pre><code>## CV SL.ranger_200_All</code></pre>
<pre><code>## Running rf_tune
## RF tuned SL:
## 
## Call:  
## SuperLearner(Y = Y, X = X, newX = newX, family = family, SL.library = grid$names,  
##     verbose = FALSE, cvControl = SuperLearner.CV.control(stratifyCV = TRUE,  
##         V = 5L), obsWeights = obsWeights) 
## 
## 
##                          Risk      Coef
## SL.ranger_2_2_All   0.1722562 0.4647002
## SL.ranger_5_2_All   0.1772081 0.0000000
## SL.ranger_15_2_All  0.1794284 0.0000000
## SL.ranger_2_5_All   0.1695573 0.5352998
## SL.ranger_5_5_All   0.1711310 0.0000000
## SL.ranger_15_5_All  0.1711502 0.0000000
## SL.ranger_2_11_All  0.1768977 0.0000000
## SL.ranger_5_11_All  0.1815464 0.0000000
## SL.ranger_15_11_All 0.1755192 0.0000000</code></pre>
<pre><code>## CV rf_tune_All</code></pre>
<pre><code>## CV SL.dbarts_200_All</code></pre>
<pre><code>## Running xgb_tune
## XGB tuned SL:
## 
## Call:  
## SuperLearner(Y = Y, X = X, newX = newX, family = family, SL.library = grid2$names,  
##     verbose = FALSE, cvControl = SuperLearner.CV.control(stratifyCV = TRUE,  
##         V = 5L), obsWeights = obsWeights) 
## 
## 
##                                      Risk Coef
## SL.xgboost_fast_250_2_0.05_All  0.2479737    1
## SL.xgboost_fast_1000_2_0.05_All 0.2479737    0
## SL.xgboost_fast_250_4_0.05_All  0.2479737    0
## SL.xgboost_fast_1000_4_0.05_All 0.2479737    0
## SL.xgboost_fast_250_2_0.2_All   0.2479737    0
## SL.xgboost_fast_1000_2_0.2_All  0.2479737    0
## SL.xgboost_fast_250_4_0.2_All   0.2479737    0
## SL.xgboost_fast_1000_4_0.2_All  0.2479737    0</code></pre>
<pre><code>## CV xgb_tune_All</code></pre>
<pre><code>## CV SL.xgboost_fast_All</code></pre>
<pre><code>## Number of covariates in All is: 33</code></pre>
<pre><code>## CV SL.mean_All</code></pre>
<pre><code>## Warning in predict.lm(fit, newdata = newX, type = &quot;response&quot;): prediction from a
## rank-deficient fit may be misleading</code></pre>
<pre><code>## CV SL.lm2_All</code></pre>
<pre><code>## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

## Warning: prediction from a rank-deficient fit may be misleading</code></pre>
<pre><code>## CV SL.glm2_All</code></pre>
<pre><code>## CV SL.glmnet_fast_All</code></pre>
<pre><code>## Warning in newton(lsp = lsp, X = G$X, y = G$y, Eb = G$Eb, UrS = G$UrS, L =
## G$L, : Fitting terminated with step failure - check results carefully</code></pre>
<pre><code>## CV SL.mgcv2_1_All</code></pre>
<pre><code>## Running rpart_tune
## Rpart tuned SL:
## 
## Call:  
## SuperLearner(Y = Y, X = X, newX = newX, family = family, SL.library = grid$names,  
##     verbose = FALSE, cvControl = SuperLearner.CV.control(stratifyCV = TRUE,  
##         V = 5L), obsWeights = obsWeights) 
## 
## 
##                               Risk      Coef
## SL.rpart2_0_10_5_All     0.1621272 0.4447428
## SL.rpart2_0.01_10_5_All  0.1621272 0.0000000
## SL.rpart2_0_20_5_All     0.1584462 0.3910999
## SL.rpart2_0.01_20_5_All  0.1584462 0.0000000
## SL.rpart2_0_80_5_All     0.2485257 0.1641574
## SL.rpart2_0.01_80_5_All  0.2485257 0.0000000
## SL.rpart2_0_10_15_All    0.1621272 0.0000000
## SL.rpart2_0.01_10_15_All 0.1621272 0.0000000
## SL.rpart2_0_20_15_All    0.1584462 0.0000000
## SL.rpart2_0.01_20_15_All 0.1584462 0.0000000
## SL.rpart2_0_80_15_All    0.2485257 0.0000000
## SL.rpart2_0.01_80_15_All 0.2485257 0.0000000</code></pre>
<pre><code>## CV rpart_tune_All</code></pre>
<pre><code>## CV SL.ranger_200_All</code></pre>
<pre><code>## Running rf_tune
## RF tuned SL:
## 
## Call:  
## SuperLearner(Y = Y, X = X, newX = newX, family = family, SL.library = grid$names,  
##     verbose = FALSE, cvControl = SuperLearner.CV.control(stratifyCV = TRUE,  
##         V = 5L), obsWeights = obsWeights) 
## 
## 
##                          Risk      Coef
## SL.ranger_2_2_All   0.1531595 0.3953748
## SL.ranger_5_2_All   0.1550964 0.0000000
## SL.ranger_15_2_All  0.1632329 0.0000000
## SL.ranger_2_5_All   0.1532677 0.1431235
## SL.ranger_5_5_All   0.1522627 0.0377830
## SL.ranger_15_5_All  0.1512748 0.4237186
## SL.ranger_2_11_All  0.1643614 0.0000000
## SL.ranger_5_11_All  0.1578598 0.0000000
## SL.ranger_15_11_All 0.1583078 0.0000000</code></pre>
<pre><code>## CV rf_tune_All</code></pre>
<pre><code>## CV SL.dbarts_200_All</code></pre>
<pre><code>## Running xgb_tune
## XGB tuned SL:
## 
## Call:  
## SuperLearner(Y = Y, X = X, newX = newX, family = family, SL.library = grid2$names,  
##     verbose = FALSE, cvControl = SuperLearner.CV.control(stratifyCV = TRUE,  
##         V = 5L), obsWeights = obsWeights) 
## 
## 
##                                      Risk Coef
## SL.xgboost_fast_250_2_0.05_All  0.2485257    0
## SL.xgboost_fast_1000_2_0.05_All 0.2485257    0
## SL.xgboost_fast_250_4_0.05_All  0.2485257    0
## SL.xgboost_fast_1000_4_0.05_All 0.2485257    0
## SL.xgboost_fast_250_2_0.2_All   0.2485257    1
## SL.xgboost_fast_1000_2_0.2_All  0.2485257    0
## SL.xgboost_fast_250_4_0.2_All   0.2485257    0
## SL.xgboost_fast_1000_4_0.2_All  0.2485257    0</code></pre>
<pre><code>## CV xgb_tune_All</code></pre>
<pre><code>## CV SL.xgboost_fast_All</code></pre>
<pre><code>## Non-Negative least squares convergence: TRUE</code></pre>
<pre><code>## full SL.mean_All</code></pre>
<pre><code>## Warning in predict.lm(fit, newdata = newX, type = &quot;response&quot;): prediction from a
## rank-deficient fit may be misleading</code></pre>
<pre><code>## full SL.lm2_All</code></pre>
<pre><code>## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == :
## prediction from a rank-deficient fit may be misleading</code></pre>
<pre><code>## full SL.glm2_All</code></pre>
<pre><code>## full SL.glmnet_fast_All</code></pre>
<pre><code>## Warning in newton(lsp = lsp, X = G$X, y = G$y, Eb = G$Eb, UrS = G$UrS, L =
## G$L, : Fitting terminated with step failure - check results carefully</code></pre>
<pre><code>## full SL.mgcv2_1_All</code></pre>
<pre><code>## Running rpart_tune
## Rpart tuned SL:
## 
## Call:  
## SuperLearner(Y = Y, X = X, newX = newX, family = family, SL.library = grid$names,  
##     verbose = FALSE, cvControl = SuperLearner.CV.control(stratifyCV = TRUE,  
##         V = 5L), obsWeights = obsWeights) 
## 
## 
##                               Risk      Coef
## SL.rpart2_0_10_5_All     0.1976905 0.2587054
## SL.rpart2_0.01_10_5_All  0.1976905 0.0000000
## SL.rpart2_0_20_5_All     0.1626327 0.3926556
## SL.rpart2_0.01_20_5_All  0.1626327 0.0000000
## SL.rpart2_0_80_5_All     0.1696390 0.3486390
## SL.rpart2_0.01_80_5_All  0.1696390 0.0000000
## SL.rpart2_0_10_15_All    0.2003170 0.0000000
## SL.rpart2_0.01_10_15_All 0.2003170 0.0000000
## SL.rpart2_0_20_15_All    0.1626327 0.0000000
## SL.rpart2_0.01_20_15_All 0.1626327 0.0000000
## SL.rpart2_0_80_15_All    0.1696390 0.0000000
## SL.rpart2_0.01_80_15_All 0.1696390 0.0000000</code></pre>
<pre><code>## full rpart_tune_All</code></pre>
<pre><code>## full SL.ranger_200_All</code></pre>
<pre><code>## Running rf_tune
## RF tuned SL:
## 
## Call:  
## SuperLearner(Y = Y, X = X, newX = newX, family = family, SL.library = grid$names,  
##     verbose = FALSE, cvControl = SuperLearner.CV.control(stratifyCV = TRUE,  
##         V = 5L), obsWeights = obsWeights) 
## 
## 
##                          Risk      Coef
## SL.ranger_2_2_All   0.1476916 0.0000000
## SL.ranger_5_2_All   0.1466023 0.1607768
## SL.ranger_15_2_All  0.1493329 0.0000000
## SL.ranger_2_5_All   0.1461487 0.0000000
## SL.ranger_5_5_All   0.1399401 0.8392232
## SL.ranger_15_5_All  0.1448856 0.0000000
## SL.ranger_2_11_All  0.1492044 0.0000000
## SL.ranger_5_11_All  0.1442157 0.0000000
## SL.ranger_15_11_All 0.1458699 0.0000000</code></pre>
<pre><code>## full rf_tune_All</code></pre>
<pre><code>## full SL.dbarts_200_All</code></pre>
<pre><code>## Running xgb_tune
## XGB tuned SL:
## 
## Call:  
## SuperLearner(Y = Y, X = X, newX = newX, family = family, SL.library = grid2$names,  
##     verbose = FALSE, cvControl = SuperLearner.CV.control(stratifyCV = TRUE,  
##         V = 5L), obsWeights = obsWeights) 
## 
## 
##                                      Risk Coef
## SL.xgboost_fast_250_2_0.05_All  0.1618510    1
## SL.xgboost_fast_1000_2_0.05_All 0.1619012    0
## SL.xgboost_fast_250_4_0.05_All  0.1618510    0
## SL.xgboost_fast_1000_4_0.05_All 0.1619012    0
## SL.xgboost_fast_250_2_0.2_All   0.1621650    0
## SL.xgboost_fast_1000_2_0.2_All  0.1621761    0
## SL.xgboost_fast_250_4_0.2_All   0.1621650    0
## SL.xgboost_fast_1000_4_0.2_All  0.1621761    0</code></pre>
<pre><code>## full xgb_tune_All</code></pre>
<pre><code>## full SL.xgboost_fast_All</code></pre>
<pre><code>## Number of covariates in All is: 33</code></pre>
<pre><code>## CV SL.mean_All</code></pre>
<pre><code>## Warning in predict.lm(fit, newdata = newX, type = &quot;response&quot;): prediction from a
## rank-deficient fit may be misleading</code></pre>
<pre><code>## CV SL.lm2_All</code></pre>
<pre><code>## Warning: glm.fit: algorithm did not converge</code></pre>
<pre><code>## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred</code></pre>
<pre><code>## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == :
## prediction from a rank-deficient fit may be misleading</code></pre>
<pre><code>## CV SL.glm2_All</code></pre>
<pre><code>## CV SL.glmnet_fast_All</code></pre>
<pre><code>## Warning in newton(lsp = lsp, X = G$X, y = G$y, Eb = G$Eb, UrS = G$UrS, L =
## G$L, : Fitting terminated with step failure - check results carefully</code></pre>
<pre><code>## CV SL.mgcv2_1_All</code></pre>
<pre><code>## Running rpart_tune
## Rpart tuned SL:
## 
## Call:  
## SuperLearner(Y = Y, X = X, newX = newX, family = family, SL.library = grid$names,  
##     verbose = FALSE, cvControl = SuperLearner.CV.control(stratifyCV = TRUE,  
##         V = 5L), obsWeights = obsWeights) 
## 
## 
##                               Risk      Coef
## SL.rpart2_0_10_5_All     0.2161357 0.4983707
## SL.rpart2_0.01_10_5_All  0.2161357 0.0000000
## SL.rpart2_0_20_5_All     0.2179543 0.1561771
## SL.rpart2_0.01_20_5_All  0.2179543 0.0000000
## SL.rpart2_0_80_5_All     0.2479737 0.3454522
## SL.rpart2_0.01_80_5_All  0.2479737 0.0000000
## SL.rpart2_0_10_15_All    0.2161357 0.0000000
## SL.rpart2_0.01_10_15_All 0.2161357 0.0000000
## SL.rpart2_0_20_15_All    0.2179543 0.0000000
## SL.rpart2_0.01_20_15_All 0.2179543 0.0000000
## SL.rpart2_0_80_15_All    0.2479737 0.0000000
## SL.rpart2_0.01_80_15_All 0.2479737 0.0000000</code></pre>
<pre><code>## CV rpart_tune_All</code></pre>
<pre><code>## CV SL.ranger_200_All</code></pre>
<pre><code>## Running rf_tune
## RF tuned SL:
## 
## Call:  
## SuperLearner(Y = Y, X = X, newX = newX, family = family, SL.library = grid$names,  
##     verbose = FALSE, cvControl = SuperLearner.CV.control(stratifyCV = TRUE,  
##         V = 5L), obsWeights = obsWeights) 
## 
## 
##                          Risk       Coef
## SL.ranger_2_2_All   0.1561952 0.02168039
## SL.ranger_5_2_All   0.1608264 0.00000000
## SL.ranger_15_2_All  0.1639134 0.00000000
## SL.ranger_2_5_All   0.1493137 0.00000000
## SL.ranger_5_5_All   0.1480154 0.58717520
## SL.ranger_15_5_All  0.1518538 0.00000000
## SL.ranger_2_11_All  0.1482667 0.39114441
## SL.ranger_5_11_All  0.1494761 0.00000000
## SL.ranger_15_11_All 0.1631246 0.00000000</code></pre>
<pre><code>## CV rf_tune_All</code></pre>
<pre><code>## CV SL.dbarts_200_All</code></pre>
<pre><code>## Running xgb_tune
## XGB tuned SL:
## 
## Call:  
## SuperLearner(Y = Y, X = X, newX = newX, family = family, SL.library = grid2$names,  
##     verbose = FALSE, cvControl = SuperLearner.CV.control(stratifyCV = TRUE,  
##         V = 5L), obsWeights = obsWeights) 
## 
## 
##                                      Risk Coef
## SL.xgboost_fast_250_2_0.05_All  0.2479737    1
## SL.xgboost_fast_1000_2_0.05_All 0.2479737    0
## SL.xgboost_fast_250_4_0.05_All  0.2479737    0
## SL.xgboost_fast_1000_4_0.05_All 0.2479737    0
## SL.xgboost_fast_250_2_0.2_All   0.2479737    0
## SL.xgboost_fast_1000_2_0.2_All  0.2479737    0
## SL.xgboost_fast_250_4_0.2_All   0.2479737    0
## SL.xgboost_fast_1000_4_0.2_All  0.2479737    0</code></pre>
<pre><code>## CV xgb_tune_All</code></pre>
<pre><code>## CV SL.xgboost_fast_All</code></pre>
<pre><code>## Number of covariates in All is: 33</code></pre>
<pre><code>## CV SL.mean_All</code></pre>
<pre><code>## Warning in predict.lm(fit, newdata = newX, type = &quot;response&quot;): prediction from a
## rank-deficient fit may be misleading</code></pre>
<pre><code>## CV SL.lm2_All</code></pre>
<pre><code>## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == :
## prediction from a rank-deficient fit may be misleading</code></pre>
<pre><code>## CV SL.glm2_All</code></pre>
<pre><code>## CV SL.glmnet_fast_All</code></pre>
<pre><code>## Warning in newton(lsp = lsp, X = G$X, y = G$y, Eb = G$Eb, UrS = G$UrS, L =
## G$L, : Fitting terminated with step failure - check results carefully</code></pre>
<pre><code>## CV SL.mgcv2_1_All</code></pre>
<pre><code>## Running rpart_tune
## Rpart tuned SL:
## 
## Call:  
## SuperLearner(Y = Y, X = X, newX = newX, family = family, SL.library = grid$names,  
##     verbose = FALSE, cvControl = SuperLearner.CV.control(stratifyCV = TRUE,  
##         V = 5L), obsWeights = obsWeights) 
## 
## 
##                               Risk      Coef
## SL.rpart2_0_10_5_All     0.2410372 0.1299163
## SL.rpart2_0.01_10_5_All  0.2410372 0.0000000
## SL.rpart2_0_20_5_All     0.1972847 0.5981439
## SL.rpart2_0.01_20_5_All  0.1972847 0.0000000
## SL.rpart2_0_80_5_All     0.2480521 0.2719398
## SL.rpart2_0.01_80_5_All  0.2480521 0.0000000
## SL.rpart2_0_10_15_All    0.2410372 0.0000000
## SL.rpart2_0.01_10_15_All 0.2410372 0.0000000
## SL.rpart2_0_20_15_All    0.1972847 0.0000000
## SL.rpart2_0.01_20_15_All 0.1972847 0.0000000
## SL.rpart2_0_80_15_All    0.2480521 0.0000000
## SL.rpart2_0.01_80_15_All 0.2480521 0.0000000</code></pre>
<pre><code>## CV rpart_tune_All</code></pre>
<pre><code>## CV SL.ranger_200_All</code></pre>
<pre><code>## Running rf_tune
## RF tuned SL:
## 
## Call:  
## SuperLearner(Y = Y, X = X, newX = newX, family = family, SL.library = grid$names,  
##     verbose = FALSE, cvControl = SuperLearner.CV.control(stratifyCV = TRUE,  
##         V = 5L), obsWeights = obsWeights) 
## 
## 
##                          Risk       Coef
## SL.ranger_2_2_All   0.1734513 0.00000000
## SL.ranger_5_2_All   0.1723206 0.00000000
## SL.ranger_15_2_All  0.1747325 0.00000000
## SL.ranger_2_5_All   0.1574738 0.00000000
## SL.ranger_5_5_All   0.1536814 0.00000000
## SL.ranger_15_5_All  0.1614467 0.00000000
## SL.ranger_2_11_All  0.1502900 0.94146009
## SL.ranger_5_11_All  0.1523159 0.05853991
## SL.ranger_15_11_All 0.1528681 0.00000000</code></pre>
<pre><code>## CV rf_tune_All</code></pre>
<pre><code>## CV SL.dbarts_200_All</code></pre>
<pre><code>## Running xgb_tune
## XGB tuned SL:
## 
## Call:  
## SuperLearner(Y = Y, X = X, newX = newX, family = family, SL.library = grid2$names,  
##     verbose = FALSE, cvControl = SuperLearner.CV.control(stratifyCV = TRUE,  
##         V = 5L), obsWeights = obsWeights) 
## 
## 
##                                      Risk Coef
## SL.xgboost_fast_250_2_0.05_All  0.2480521    0
## SL.xgboost_fast_1000_2_0.05_All 0.2480521    0
## SL.xgboost_fast_250_4_0.05_All  0.2480521    0
## SL.xgboost_fast_1000_4_0.05_All 0.2480521    0
## SL.xgboost_fast_250_2_0.2_All   0.2480521    1
## SL.xgboost_fast_1000_2_0.2_All  0.2480521    0
## SL.xgboost_fast_250_4_0.2_All   0.2480521    0
## SL.xgboost_fast_1000_4_0.2_All  0.2480521    0</code></pre>
<pre><code>## CV xgb_tune_All</code></pre>
<pre><code>## CV SL.xgboost_fast_All</code></pre>
<pre><code>## Non-Negative least squares convergence: TRUE</code></pre>
<pre><code>## full SL.mean_All</code></pre>
<pre><code>## full SL.lm2_All</code></pre>
<pre><code>## full SL.glm2_All</code></pre>
<pre><code>## full SL.glmnet_fast_All</code></pre>
<pre><code>## Warning in newton(lsp = lsp, X = G$X, y = G$y, Eb = G$Eb, UrS = G$UrS, L =
## G$L, : Fitting terminated with step failure - check results carefully</code></pre>
<pre><code>## full SL.mgcv2_1_All</code></pre>
<pre><code>## Running rpart_tune
## Rpart tuned SL:
## 
## Call:  
## SuperLearner(Y = Y, X = X, newX = newX, family = family, SL.library = grid$names,  
##     verbose = FALSE, cvControl = SuperLearner.CV.control(stratifyCV = TRUE,  
##         V = 5L), obsWeights = obsWeights) 
## 
## 
##                               Risk      Coef
## SL.rpart2_0_10_5_All     0.2224368 0.0000000
## SL.rpart2_0.01_10_5_All  0.2164228 0.0000000
## SL.rpart2_0_20_5_All     0.2044387 0.3738306
## SL.rpart2_0.01_20_5_All  0.2044387 0.0000000
## SL.rpart2_0_80_5_All     0.2205699 0.1502646
## SL.rpart2_0.01_80_5_All  0.2205699 0.0000000
## SL.rpart2_0_10_15_All    0.2166666 0.0000000
## SL.rpart2_0.01_10_15_All 0.2062283 0.4759048
## SL.rpart2_0_20_15_All    0.2044387 0.0000000
## SL.rpart2_0.01_20_15_All 0.2044387 0.0000000
## SL.rpart2_0_80_15_All    0.2205699 0.0000000
## SL.rpart2_0.01_80_15_All 0.2205699 0.0000000</code></pre>
<pre><code>## full rpart_tune_All</code></pre>
<pre><code>## full SL.ranger_200_All</code></pre>
<pre><code>## Running rf_tune
## RF tuned SL:
## 
## Call:  
## SuperLearner(Y = Y, X = X, newX = newX, family = family, SL.library = grid$names,  
##     verbose = FALSE, cvControl = SuperLearner.CV.control(stratifyCV = TRUE,  
##         V = 5L), obsWeights = obsWeights) 
## 
## 
##                          Risk Coef
## SL.ranger_2_2_All   0.1517196    0
## SL.ranger_5_2_All   0.1537403    0
## SL.ranger_15_2_All  0.1558348    0
## SL.ranger_2_5_All   0.1407232    1
## SL.ranger_5_5_All   0.1441005    0
## SL.ranger_15_5_All  0.1464206    0
## SL.ranger_2_11_All  0.1475427    0
## SL.ranger_5_11_All  0.1459467    0
## SL.ranger_15_11_All 0.1471347    0</code></pre>
<pre><code>## full rf_tune_All</code></pre>
<pre><code>## full SL.dbarts_200_All</code></pre>
<pre><code>## Running xgb_tune
## XGB tuned SL:
## 
## Call:  
## SuperLearner(Y = Y, X = X, newX = newX, family = family, SL.library = grid2$names,  
##     verbose = FALSE, cvControl = SuperLearner.CV.control(stratifyCV = TRUE,  
##         V = 5L), obsWeights = obsWeights) 
## 
## 
##                                      Risk      Coef
## SL.xgboost_fast_250_2_0.05_All  0.1772249 0.0000000
## SL.xgboost_fast_1000_2_0.05_All 0.1773171 0.0000000
## SL.xgboost_fast_250_4_0.05_All  0.1772249 0.6528516
## SL.xgboost_fast_1000_4_0.05_All 0.1773171 0.0000000
## SL.xgboost_fast_250_2_0.2_All   0.1773340 0.0000000
## SL.xgboost_fast_1000_2_0.2_All  0.1773277 0.3471484
## SL.xgboost_fast_250_4_0.2_All   0.1773340 0.0000000
## SL.xgboost_fast_1000_4_0.2_All  0.1773277 0.0000000</code></pre>
<pre><code>## full xgb_tune_All</code></pre>
<pre><code>## full SL.xgboost_fast_All</code></pre>
<pre><code>## 
## Call:  
## CV.SuperLearner(Y = data[[vars$outcomes[1]]], X = data[, vars$predictors],  
##     family = binomial(), SL.library = sl_lib, verbose = TRUE, cvControl = SuperLearner.CV.control(stratifyCV = TRUE,  
##         V = outer_cv_folds), innerCvControl = rep(list(SuperLearner.CV.control(stratifyCV = TRUE,  
##         V = 2L)), outer_cv_folds)) 
## 
## 
## Cross-validated predictions from the SuperLearner:  SL.predict 
## 
## Cross-validated predictions from the discrete super learner (cross-validation selector):  discreteSL.predict 
## 
## Which library algorithm was the discrete super learner:  whichDiscreteSL 
## 
## Cross-validated prediction for all algorithms in the library:  library.predict</code></pre>
<pre class="r"><code>save(cvsl,
     file = &quot;data/estimator-cvsl.RData&quot;)

summary(cvsl)</code></pre>
<pre><code>## 
## Call:  
## CV.SuperLearner(Y = data[[vars$outcomes[1]]], X = data[, vars$predictors],  
##     family = binomial(), SL.library = sl_lib, verbose = TRUE, cvControl = SuperLearner.CV.control(stratifyCV = TRUE,  
##         V = outer_cv_folds), innerCvControl = rep(list(SuperLearner.CV.control(stratifyCV = TRUE,  
##         V = 2L)), outer_cv_folds)) 
## 
## Risk is based on: Mean Squared Error
## 
## All risk estimates are based on V =  2 
## 
##            Algorithm     Ave        se     Min     Max
##        Super Learner 0.12301 0.0105095 0.11428 0.13174
##          Discrete SL 0.12858 0.0104708 0.11691 0.14026
##          SL.mean_All 0.24802 0.0025546 0.24789 0.24816
##           SL.lm2_All 0.12348 0.0111081 0.11796 0.12899
##          SL.glm2_All 0.15932 0.0175901 0.13357 0.18506
##   SL.glmnet_fast_All 0.12033 0.0118126 0.11423 0.12644
##       SL.mgcv2_1_All 0.21078 0.0216640 0.20320 0.21836
##       rpart_tune_All 0.17977 0.0160481 0.17587 0.18367
##    SL.ranger_200_All 0.13516 0.0102176 0.13006 0.14026
##          rf_tune_All 0.13666 0.0097767 0.12989 0.14343
##    SL.dbarts_200_All 0.12072 0.0102435 0.11691 0.12454
##         xgb_tune_All 0.15189 0.0125132 0.13988 0.16390
##  SL.xgboost_fast_All 0.15483 0.0128581 0.13984 0.16982</code></pre>
<div id="review-cv.sl" class="section level3">
<h3>Review CV.SL</h3>
<pre class="r"><code>library(dplyr)
library(ck37r)

######
# AUC analysis

(auc_tab = auc_table(cvsl))</code></pre>
<pre><code>##                           auc         se  ci_lower  ci_upper      p-value
## SL.mean_All         0.5000000 0.05767800 0.3869532 0.6130468 5.572784e-13
## SL.mgcv2_1_All      0.7926052 0.03489829 0.7242058 0.8610046 3.681344e-04
## rpart_tune_All      0.8021050 0.03265101 0.7381102 0.8660998 4.547933e-04
## SL.glm2_All         0.8500023 0.02405759 0.8028503 0.8971544 6.019082e-03
## SL.xgboost_fast_All 0.8513109 0.02223601 0.8077291 0.8948926 3.931687e-03
## xgb_tune_All        0.8544592 0.02194894 0.8114401 0.8974783 5.398441e-03
## rf_tune_All         0.8899990 0.01830823 0.8541156 0.9258825 1.324389e-01
## SL.ranger_200_All   0.8923691 0.01797045 0.8571476 0.9275905 1.576874e-01
## DiscreteSL          0.8971411 0.01796886 0.8619227 0.9323594 2.300974e-01
## SL.glmnet_fast_All  0.9021138 0.01826855 0.8663081 0.9379194 3.248385e-01
## SL.lm2_All          0.9036720 0.01845892 0.8674931 0.9398508 3.575138e-01
## SuperLearner        0.9042959 0.01751301 0.8699710 0.9386207 3.634671e-01
## SL.dbarts_200_All   0.9104115 0.01685095 0.8773843 0.9434388 5.000000e-01</code></pre>
<pre class="r"><code># Drop p-value column.
auc_tab2 = auc_tab[, !names(auc_tab) %in% &quot;p-value&quot;]

# Convert rownames to learner column.
auc_tab2$learner = rownames(auc_tab2)

# Move learner column to the beginning.
(auc_tab2 = cbind(learner = auc_tab2$learner,
                  auc_tab2[, !names(auc_tab2) %in% &quot;learner&quot;]))</code></pre>
<pre><code>##                                 learner       auc         se  ci_lower
## SL.mean_All                 SL.mean_All 0.5000000 0.05767800 0.3869532
## SL.mgcv2_1_All           SL.mgcv2_1_All 0.7926052 0.03489829 0.7242058
## rpart_tune_All           rpart_tune_All 0.8021050 0.03265101 0.7381102
## SL.glm2_All                 SL.glm2_All 0.8500023 0.02405759 0.8028503
## SL.xgboost_fast_All SL.xgboost_fast_All 0.8513109 0.02223601 0.8077291
## xgb_tune_All               xgb_tune_All 0.8544592 0.02194894 0.8114401
## rf_tune_All                 rf_tune_All 0.8899990 0.01830823 0.8541156
## SL.ranger_200_All     SL.ranger_200_All 0.8923691 0.01797045 0.8571476
## DiscreteSL                   DiscreteSL 0.8971411 0.01796886 0.8619227
## SL.glmnet_fast_All   SL.glmnet_fast_All 0.9021138 0.01826855 0.8663081
## SL.lm2_All                   SL.lm2_All 0.9036720 0.01845892 0.8674931
## SuperLearner               SuperLearner 0.9042959 0.01751301 0.8699710
## SL.dbarts_200_All     SL.dbarts_200_All 0.9104115 0.01685095 0.8773843
##                      ci_upper
## SL.mean_All         0.6130468
## SL.mgcv2_1_All      0.8610046
## rpart_tune_All      0.8660998
## SL.glm2_All         0.8971544
## SL.xgboost_fast_All 0.8948926
## xgb_tune_All        0.8974783
## rf_tune_All         0.9258825
## SL.ranger_200_All   0.9275905
## DiscreteSL          0.9323594
## SL.glmnet_fast_All  0.9379194
## SL.lm2_All          0.9398508
## SuperLearner        0.9386207
## SL.dbarts_200_All   0.9434388</code></pre>
<pre class="r"><code>colnames(auc_tab2)[1] = &quot;learner&quot;
rownames(auc_tab2) = NULL

# Skip SL.mean - it&#39;s too low to be worth plotting.
plot_table(auc_tab2[-1, ]) + labs(y = &quot;Cross-validated ROC-AUC&quot;)</code></pre>
<p><img src="5-modeling_files/figure-html/review_cvsl-1.png" width="672" /></p>
<pre class="r"><code>ggsave(&quot;visuals/cvsl-roc-auc-comparison.pdf&quot;)</code></pre>
<pre><code>## Saving 7 x 5 in image</code></pre>
<pre class="r"><code>plot_roc(cvsl)</code></pre>
<p><img src="5-modeling_files/figure-html/review_cvsl-2.png" width="672" /></p>
<pre class="r"><code>ggsave(&quot;visuals/cvsl-roc.pdf&quot;)</code></pre>
<pre><code>## Saving 7 x 5 in image</code></pre>
<pre class="r"><code>names(auc_tab2)</code></pre>
<pre><code>## [1] &quot;learner&quot;  &quot;auc&quot;      &quot;se&quot;       &quot;ci_lower&quot; &quot;ci_upper&quot;</code></pre>
<pre class="r"><code>names(auc_tab2) = c(&quot;Learner&quot;, &quot;ROC-AUC&quot;, &quot;Std. Err.&quot;, &quot;CI Lower&quot;, &quot;CI Upper&quot;)

cat(kable(auc_tab2, digits = 4, format = &quot;latex&quot;, booktabs = TRUE,
          label = &quot;cvsl-auc&quot;,
          caption = &quot;Cross-validated ROC-AUC discrimination performance&quot;),
      file = &quot;tables/cvsl-auc_table.tex&quot;)


######
# Precision-Recall analysis

(prauc_tab = prauc_table(cvsl))</code></pre>
<pre><code>##                         prauc      stderr  ci_lower  ci_upper
## SL.mean_All         0.5445495 0.001503137 0.5416033 0.5474956
## SL.mgcv2_1_All      0.7583945 0.016486244 0.7260815 0.7907075
## rpart_tune_All      0.8018188 0.020264132 0.7621011 0.8415365
## SL.glm2_All         0.8060125 0.004788288 0.7966275 0.8153976
## SL.xgboost_fast_All 0.8608036 0.001473147 0.8579162 0.8636910
## xgb_tune_All        0.8658081 0.001702237 0.8624717 0.8691445
## SL.glmnet_fast_All  0.9030168 0.017109101 0.8694829 0.9365506
## SL.lm2_All          0.9055796 0.013157978 0.8797900 0.9313693
## rf_tune_All         0.9073990 0.004783451 0.8980234 0.9167745
## DiscreteSL          0.9079113 0.007564940 0.8930840 0.9227386
## SL.ranger_200_All   0.9109092 0.004566986 0.9019579 0.9198605
## SuperLearner        0.9124573 0.014727472 0.8835915 0.9413232
## SL.dbarts_200_All   0.9186478 0.018301425 0.8827770 0.9545186</code></pre>
<pre class="r"><code>prauc_tab$learner = rownames(prauc_tab)
rownames(prauc_tab) = NULL

# Move learner column to the beginning.
(prauc_tab = cbind(learner = prauc_tab$learner,
                   prauc_tab[, !names(prauc_tab) %in% &quot;learner&quot;]))</code></pre>
<pre><code>##                learner     prauc      stderr  ci_lower  ci_upper
## 1          SL.mean_All 0.5445495 0.001503137 0.5416033 0.5474956
## 2       SL.mgcv2_1_All 0.7583945 0.016486244 0.7260815 0.7907075
## 3       rpart_tune_All 0.8018188 0.020264132 0.7621011 0.8415365
## 4          SL.glm2_All 0.8060125 0.004788288 0.7966275 0.8153976
## 5  SL.xgboost_fast_All 0.8608036 0.001473147 0.8579162 0.8636910
## 6         xgb_tune_All 0.8658081 0.001702237 0.8624717 0.8691445
## 7   SL.glmnet_fast_All 0.9030168 0.017109101 0.8694829 0.9365506
## 8           SL.lm2_All 0.9055796 0.013157978 0.8797900 0.9313693
## 9          rf_tune_All 0.9073990 0.004783451 0.8980234 0.9167745
## 10          DiscreteSL 0.9079113 0.007564940 0.8930840 0.9227386
## 11   SL.ranger_200_All 0.9109092 0.004566986 0.9019579 0.9198605
## 12        SuperLearner 0.9124573 0.014727472 0.8835915 0.9413232
## 13   SL.dbarts_200_All 0.9186478 0.018301425 0.8827770 0.9545186</code></pre>
<pre class="r"><code>plot_table(prauc_tab, metric = &quot;prauc&quot;) + labs(y = &quot;Cross-validated PR-AUC&quot;)</code></pre>
<p><img src="5-modeling_files/figure-html/review_cvsl-3.png" width="672" /></p>
<pre class="r"><code>ggsave(&quot;visuals/cvsl-prauc-comparison.pdf&quot;)</code></pre>
<pre><code>## Saving 7 x 5 in image</code></pre>
<pre class="r"><code>################
# Precision-recall curve comparison.

pred_lib = data.frame(cvsl$library.predict)

names(pred_lib) = cvsl$libraryNames
pred_df = pred_lib

summary(pred_df)</code></pre>
<pre><code>##   SL.mean_All       SL.lm2_All      SL.glm2_All      SL.glmnet_fast_All
##  Min.   :0.5430   Min.   :0.0000   Min.   :0.00000   Min.   :0.005788  
##  1st Qu.:0.5430   1st Qu.:0.2161   1st Qu.:0.02536   1st Qu.:0.153894  
##  Median :0.5430   Median :0.5856   Median :0.66663   Median :0.595197  
##  Mean   :0.5445   Mean   :0.5377   Mean   :0.53556   Mean   :0.534704  
##  3rd Qu.:0.5461   3rd Qu.:0.8654   3rd Qu.:0.98797   3rd Qu.:0.895400  
##  Max.   :0.5461   Max.   :1.0000   Max.   :1.00000   Max.   :0.990595  
##  SL.mgcv2_1_All     rpart_tune_All   SL.ranger_200_All   rf_tune_All      
##  Min.   :0.000000   Min.   :0.1069   Min.   :0.002153   Min.   :0.003639  
##  1st Qu.:0.001971   1st Qu.:0.1187   1st Qu.:0.268224   1st Qu.:0.282871  
##  Median :0.911403   Median :0.6130   Median :0.552715   Median :0.547087  
##  Mean   :0.583414   Mean   :0.5413   Mean   :0.532664   Mean   :0.539089  
##  3rd Qu.:1.000000   3rd Qu.:0.8611   3rd Qu.:0.806259   3rd Qu.:0.807466  
##  Max.   :1.000000   Max.   :0.9697   Max.   :0.986792   Max.   :0.983903  
##  SL.dbarts_200_All  xgb_tune_All     SL.xgboost_fast_All
##  Min.   :0.01679   Min.   :0.05916   Min.   :0.04974    
##  1st Qu.:0.23077   1st Qu.:0.20063   1st Qu.:0.20410    
##  Median :0.59038   Median :0.56120   Median :0.54630    
##  Mean   :0.53763   Mean   :0.53828   Mean   :0.53559    
##  3rd Qu.:0.84723   3rd Qu.:0.86329   3rd Qu.:0.85939    
##  Max.   :0.98499   Max.   :0.95300   Max.   :0.98352</code></pre>
<pre class="r"><code># Add on the SuperLearner prediction.
pred_df$SuperLearner = cvsl$SL.predict

library(precrec)
library(ggplot2)

(sscurves = evalmod(scores = pred_df,
                    labels = cvsl$Y,
                    modnames = names(pred_df)))</code></pre>
<pre><code>## 
##     === AUCs ===
## 
##               Model name Dataset ID Curve type       AUC
##    1         SL.mean_All          1        ROC 0.4984848
##    2         SL.mean_All          1        PRC 0.5435123
##    3          SL.lm2_All          1        ROC 0.9037330
##    4          SL.lm2_All          1        PRC 0.9029695
##    5         SL.glm2_All          1        ROC 0.8492314
##    6         SL.glm2_All          1        PRC 0.8064428
##    7  SL.glmnet_fast_All          1        ROC 0.9036671
##    8  SL.glmnet_fast_All          1        PRC 0.9006941
##    9      SL.mgcv2_1_All          1        ROC 0.7780632
##   10      SL.mgcv2_1_All          1        PRC 0.7525227
##   11      rpart_tune_All          1        ROC 0.8059289
##   12      rpart_tune_All          1        PRC 0.7910480
##   13   SL.ranger_200_All          1        ROC 0.8916996
##   14   SL.ranger_200_All          1        PRC 0.9096273
##   15         rf_tune_All          1        ROC 0.8902942
##   16         rf_tune_All          1        PRC 0.9046698
##   17   SL.dbarts_200_All          1        ROC 0.9101449
##   18   SL.dbarts_200_All          1        PRC 0.9115239
##   19        xgb_tune_All          1        ROC 0.8561704
##   20        xgb_tune_All          1        PRC 0.8659110
##   21 SL.xgboost_fast_All          1        ROC 0.8520861
##   22 SL.xgboost_fast_All          1        PRC 0.8618175
##   23        SuperLearner          1        ROC 0.9055775
##   24        SuperLearner          1        PRC 0.9065846
## 
## 
##     === Input data ===
## 
##               Model name Dataset ID # of negatives # of positives
##    1         SL.mean_All          1            138            165
##    2          SL.lm2_All          1            138            165
##    3         SL.glm2_All          1            138            165
##    4  SL.glmnet_fast_All          1            138            165
##    5      SL.mgcv2_1_All          1            138            165
##    6      rpart_tune_All          1            138            165
##    7   SL.ranger_200_All          1            138            165
##    8         rf_tune_All          1            138            165
##    9   SL.dbarts_200_All          1            138            165
##   10        xgb_tune_All          1            138            165
##   11 SL.xgboost_fast_All          1            138            165
##   12        SuperLearner          1            138            165</code></pre>
<pre class="r"><code># Show a Precision-Recall plot comparing all estimators.
# TODO: subset this one and improve legend placement.
autoplot(sscurves, &quot;PRC&quot;) +
  labs(title = element_blank()) +
  theme(legend.position = c(0.65, 0.65),
        legend.text = element_text(size = 8), #face = &quot;bold&quot;),
        legend.margin = margin(l = 0.2, r = 0.2, b = 0.2, unit = &quot;cm&quot;),
        legend.background = element_rect(fill = alpha(&quot;gray95&quot;, 0.8),
                                         color = &quot;gray80&quot;),
        legend.key = element_blank())</code></pre>
<p><img src="5-modeling_files/figure-html/review_cvsl-4.png" width="672" /></p>
<pre class="r"><code>ggsave(&quot;visuals/prc-comparison.pdf&quot;,
       width = 4, height = 4)


################
# PR-AUC table
names(prauc_tab) = c(&quot;Learner&quot;, &quot;PR-AUC&quot;, &quot;Std. Err.&quot;, &quot;CI Lower&quot;, &quot;CI Upper&quot;)

cat(kable(prauc_tab, digits = 4, format = &quot;latex&quot;, booktabs = TRUE,
          label = &quot;cvsl-prauc&quot;,
          caption = &quot;Cross-validated PR-AUC discrimination performance&quot;) %&gt;%
      kable_styling(latex_options = &quot;hold_position&quot;),
      file = &quot;tables/cvsl-prauc_table.tex&quot;)


# Review weight distribution.
(weight_tab = ck37r::cvsl_weights(cvsl))</code></pre>
<pre><code>##     #      Learner    Mean      SD     Min     Max
## 1   1   dbarts_200 0.56312 0.19481 0.42538 0.70087
## 2   2   ranger_200 0.28533 0.40351 0.00000 0.57066
## 3   3  glmnet_fast 0.11442 0.16181 0.00000 0.22883
## 4   4      mgcv2_1 0.03515 0.04971 0.00000 0.07030
## 5   5         glm2 0.00198 0.00281 0.00000 0.00397
## 6   6         mean 0.00000 0.00000 0.00000 0.00000
## 7   7          lm2 0.00000 0.00000 0.00000 0.00000
## 8   8   rpart_tune 0.00000 0.00000 0.00000 0.00000
## 9   9      rf_tune 0.00000 0.00000 0.00000 0.00000
## 10 10     xgb_tune 0.00000 0.00000 0.00000 0.00000
## 11 11 xgboost_fast 0.00000 0.00000 0.00000 0.00000</code></pre>
<pre class="r"><code># Remove algorithms with 0 weight.
(weight_tab = weight_tab[weight_tab$Max &gt; 0, ])</code></pre>
<pre><code>##   #     Learner    Mean      SD     Min     Max
## 1 1  dbarts_200 0.56312 0.19481 0.42538 0.70087
## 2 2  ranger_200 0.28533 0.40351 0.00000 0.57066
## 3 3 glmnet_fast 0.11442 0.16181 0.00000 0.22883
## 4 4     mgcv2_1 0.03515 0.04971 0.00000 0.07030
## 5 5        glm2 0.00198 0.00281 0.00000 0.00397</code></pre>
<pre class="r"><code>cat(kable(weight_tab, digits = 4, format = &quot;latex&quot;, booktabs = TRUE,
          label = &quot;cvsl-weights&quot;,
          row.names = FALSE,
          caption = &quot;Distribution of algorithm weights across ensemble cross-validation replications&quot;) %&gt;%
      kable_styling(latex_options = &quot;hold_position&quot;),
      file = &quot;tables/cvsl-weight-table.tex&quot;)


##########################
# Brier score table.

(brier_tab = ck37r::brier_table(cvsl))</code></pre>
<pre><code>## Error: &#39;brier_table&#39; is not an exported object from &#39;namespace:ck37r&#39;</code></pre>
<pre class="r"><code>names(brier_tab) = c(&quot;Brier score&quot;, &quot;Std. Err.&quot;, &quot;CI Lower&quot;, &quot;CI Upper&quot;)</code></pre>
<pre><code>## Error in names(brier_tab) = c(&quot;Brier score&quot;, &quot;Std. Err.&quot;, &quot;CI Lower&quot;, : object &#39;brier_tab&#39; not found</code></pre>
<pre class="r"><code>brier_tab = cbind(Learner = rownames(brier_tab), brier_tab)</code></pre>
<pre><code>## Error in rownames(brier_tab): object &#39;brier_tab&#39; not found</code></pre>
<pre class="r"><code>rownames(brier_tab) = NULL</code></pre>
<pre><code>## Error in rownames(brier_tab) = NULL: object &#39;brier_tab&#39; not found</code></pre>
<pre class="r"><code>cat(kable(brier_tab, digits = 5, format = &quot;latex&quot;, booktabs = TRUE,
          label = &quot;cvsl-brier&quot;,
          row.names = FALSE,
          caption = &quot;Cross-validated Brier score for each learner and the ensemble&quot;) %&gt;%
      kable_styling(latex_options = &quot;hold_position&quot;),
      file = &quot;tables/cvsl-brier-table.tex&quot;)</code></pre>
<pre><code>## Error in kable(brier_tab, digits = 5, format = &quot;latex&quot;, booktabs = TRUE, : object &#39;brier_tab&#39; not found</code></pre>
<pre class="r"><code>##########################
# Index of prediction accuracy table.

(ipa_tab = ck37r::ipa_table(cvsl))</code></pre>
<pre><code>## Error: &#39;ipa_table&#39; is not an exported object from &#39;namespace:ck37r&#39;</code></pre>
<pre class="r"><code>names(ipa_tab) = c(&quot;IPA&quot;, &quot;Std. Err.&quot;, &quot;CI Lower&quot;, &quot;CI Upper&quot;)</code></pre>
<pre><code>## Error in names(ipa_tab) = c(&quot;IPA&quot;, &quot;Std. Err.&quot;, &quot;CI Lower&quot;, &quot;CI Upper&quot;): object &#39;ipa_tab&#39; not found</code></pre>
<pre class="r"><code>ipa_tab = cbind(Learner = rownames(ipa_tab), ipa_tab)</code></pre>
<pre><code>## Error in rownames(ipa_tab): object &#39;ipa_tab&#39; not found</code></pre>
<pre class="r"><code>rownames(ipa_tab) = NULL</code></pre>
<pre><code>## Error in rownames(ipa_tab) = NULL: object &#39;ipa_tab&#39; not found</code></pre>
<pre class="r"><code>cat(kable(ipa_tab, digits = 4, format = &quot;latex&quot;, booktabs = TRUE,
          label = &quot;cvsl-ipa&quot;,
          row.names = FALSE,
          caption = &quot;Cross-validated index of prediction accuracy for each learner and the ensemble&quot;) %&gt;%
      kable_styling(latex_options = &quot;hold_position&quot;),
      file = &quot;tables/cvsl-ipa-table.tex&quot;)</code></pre>
<pre><code>## Error in kable(ipa_tab, digits = 4, format = &quot;latex&quot;, booktabs = TRUE, : object &#39;ipa_tab&#39; not found</code></pre>
<p>Next file: 6-interpretation.Rmd</p>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
